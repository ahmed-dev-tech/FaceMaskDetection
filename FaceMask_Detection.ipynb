{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "FaceMask_Detection.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SgwY5Vzx6DR"
      },
      "source": [
        "## **Importing Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFI7uQd29op_"
      },
      "source": [
        "import os\n",
        "import zipfile\n",
        "import random\n",
        "import shutil\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from shutil import copyfile\n",
        "from os import getcwd\n",
        "from os import listdir\n",
        "import cv2\n",
        "from tensorflow.keras.layers import Conv2D, Input, ZeroPadding2D, BatchNormalization, Activation, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.utils import shuffle\n",
        "import imutils\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image  as mpimg\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing import image\n",
        "import datetime"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hX8eel9vxsP5"
      },
      "source": [
        "### DataSet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vj0UroXP9oqW",
        "outputId": "a4c27ba3-d9fe-4933-93a0-465d5a147ddf"
      },
      "source": [
        "print(\"The number of images with facemask labelled 'yes':\",len(os.listdir('/data/train/with_mask')))\n",
        "print(\"The number of images with facemask labelled 'no':\",len(os.listdir('/data/train/with_mask')))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The number of images with facemask labelled 'yes': 690\n",
            "The number of images with facemask labelled 'no': 686\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSsMseTT9oqb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9599b5de-8601-4159-c1d5-802051eeb723"
      },
      "source": [
        "def data_summary(main_path):\n",
        "    \n",
        "    yes_path = main_path+'with_mask'\n",
        "    no_path = main_path+'without_mask'\n",
        "        \n",
        "    # number of files (images) that are in the the folder named 'yes' that represent tumorous (positive) examples\n",
        "    m_pos = len(listdir(yes_path))\n",
        "    # number of files (images) that are in the the folder named 'no' that represent non-tumorous (negative) examples\n",
        "    m_neg = len(listdir(no_path))\n",
        "    # number of all examples\n",
        "    m = (m_pos+m_neg)\n",
        "    \n",
        "    pos_prec = (m_pos* 100.0)/ m\n",
        "    neg_prec = (m_neg* 100.0)/ m\n",
        "    \n",
        "    print(f\"Number of examples: {m}\")\n",
        "    print(f\"Percentage of positive examples: {pos_prec}%, number of pos examples: {m_pos}\") \n",
        "    print(f\"Percentage of negative examples: {neg_prec}%, number of neg examples: {m_neg}\") \n",
        "    \n",
        "augmented_data_path = '//data/train/'    \n",
        "data_summary(augmented_data_path)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of examples: 1508\n",
            "Percentage of positive examples: 50.06631299734748%, number of pos examples: 755\n",
            "Percentage of negative examples: 49.93368700265252%, number of neg examples: 753\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4IIkTcAzQN0"
      },
      "source": [
        "## **Data** **Spliting**\n",
        "80% training data\n",
        "20% testing data \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23-l-9l79oqc"
      },
      "source": [
        "def split_data(SOURCE, TRAINING, TESTING, SPLIT_SIZE):\n",
        "    dataset = []\n",
        "    \n",
        "    for unitData in os.listdir(SOURCE):\n",
        "        data = SOURCE + unitData\n",
        "        if(os.path.getsize(data) > 0):\n",
        "            dataset.append(unitData)\n",
        "        else:\n",
        "            print('Skipped ' + unitData)\n",
        "            print('Invalid file i.e zero size')\n",
        "    \n",
        "    train_set_length = int(len(dataset) * SPLIT_SIZE)\n",
        "    test_set_length = int(len(dataset) - train_set_length)\n",
        "    shuffled_set = random.sample(dataset, len(dataset))\n",
        "    train_set = dataset[0:train_set_length]\n",
        "    test_set = dataset[-test_set_length:]\n",
        "       \n",
        "    for unitData in train_set:\n",
        "        temp_train_set = SOURCE + unitData\n",
        "        final_train_set = TRAINING + unitData\n",
        "        copyfile(temp_train_set, final_train_set)\n",
        "    \n",
        "    for unitData in test_set:\n",
        "        temp_test_set = SOURCE + unitData\n",
        "        final_test_set = TESTING + unitData\n",
        "        copyfile(temp_test_set, final_test_set)\n",
        "        \n",
        "        \n",
        "YES_SOURCE_DIR = \"/data/with_mask/\"\n",
        "TRAINING_YES_DIR = \"/data/train/with_mask/\"\n",
        "TESTING_YES_DIR = \"/data/test/with_mask/\"\n",
        "NO_SOURCE_DIR =  \"/data/without_mask/\"\n",
        "TRAINING_NO_DIR = \"/data/train/without_mask/\"\n",
        "TESTING_NO_DIR =  \"/data/test/without_mask/\"\n",
        "split_size = .8\n",
        "split_data(YES_SOURCE_DIR, TRAINING_YES_DIR, TESTING_YES_DIR, split_size)\n",
        "split_data(NO_SOURCE_DIR, TRAINING_NO_DIR, TESTING_NO_DIR, split_size)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hk-MTIcm0LLx"
      },
      "source": [
        "Describtion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYUk9Upy9oqe"
      },
      "source": [
        "print(\"The number of images with facemask in the training set labelled 'yes':\", len(os.listdir('/data/train/with_mask')))\n",
        "print(\"The number of images with facemask in the test set labelled 'yes':\", len(os.listdir('/data/test/with_mask/')))\n",
        "print(\"The number of images without facemask in the training set labelled 'no':\", len(os.listdir('/data/train/without_mask/')))\n",
        "print(\"The number of images without facemask in the test set labelled 'no':\", len(os.listdir('/data/test/without_mask/')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nidPlpAt0g5H"
      },
      "source": [
        "### CNN Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2nz56gZ9oqg"
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(100, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    \n",
        "    tf.keras.layers.Conv2D(100, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    \n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(50, activation='relu'),\n",
        "    tf.keras.layers.Dense(2, activation='softmax')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "neFF_2OX9oqh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc0146e4-96a2-4f1f-8cdf-e25e4261ea95"
      },
      "source": [
        "TRAINING_DIR =\"/data/train/\"\n",
        "train_datagen = ImageDataGenerator(rescale=1.0/255,\n",
        "                                   rotation_range=40,\n",
        "                                   width_shift_range=0.2,\n",
        "                                   height_shift_range=0.2,\n",
        "                                   shear_range=0.2,\n",
        "                                   zoom_range=0.2,\n",
        "                                   horizontal_flip=True,\n",
        "                                   fill_mode='nearest')\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(TRAINING_DIR, \n",
        "                                                    batch_size=10, \n",
        "                                                    target_size=(150, 150))\n",
        "VALIDATION_DIR = \"/data/test/\"\n",
        "validation_datagen = ImageDataGenerator(rescale=1.0/255)\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(VALIDATION_DIR, \n",
        "                                                         batch_size=10, \n",
        "                                                         target_size=(150, 150))\n",
        "checkpoint = ModelCheckpoint('model-{epoch:03d}.model',monitor='val_loss',verbose=0,save_best_only=True,mode='auto')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1508 images belonging to 2 classes.\n",
            "Found 470 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxAQY1MJKIfj",
        "outputId": "467472ed-dc0a-4b23-a69b-d0911329b06e"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 148, 148, 100)     2800      \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 74, 74, 100)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 72, 72, 100)       90100     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 36, 36, 100)       0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 129600)            0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 129600)            0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 50)                6480050   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 2)                 102       \n",
            "=================================================================\n",
            "Total params: 6,573,052\n",
            "Trainable params: 6,573,052\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WemBavlN04S8"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbvDToZm9oqj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48c4e36b-3393-4486-b033-205e2679a56e"
      },
      "source": [
        "history = model.fit_generator(train_generator,\n",
        "                              epochs=30,\n",
        "                              validation_data=validation_generator,\n",
        "                              callbacks=[checkpoint])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "151/151 [==============================] - 269s 2s/step - loss: 0.7237 - acc: 0.5794 - val_loss: 0.4607 - val_acc: 0.8936\n",
            "INFO:tensorflow:Assets written to: model-001.model/assets\n",
            "Epoch 2/30\n",
            "151/151 [==============================] - 144s 956ms/step - loss: 0.4969 - acc: 0.8493 - val_loss: 0.3727 - val_acc: 0.9362\n",
            "INFO:tensorflow:Assets written to: model-002.model/assets\n",
            "Epoch 3/30\n",
            "151/151 [==============================] - 145s 962ms/step - loss: 0.4311 - acc: 0.8892 - val_loss: 0.3379 - val_acc: 0.9489\n",
            "INFO:tensorflow:Assets written to: model-003.model/assets\n",
            "Epoch 4/30\n",
            "151/151 [==============================] - 144s 954ms/step - loss: 0.4332 - acc: 0.8688 - val_loss: 0.3871 - val_acc: 0.8915\n",
            "Epoch 5/30\n",
            "151/151 [==============================] - 144s 951ms/step - loss: 0.3814 - acc: 0.8978 - val_loss: 0.3159 - val_acc: 0.9404\n",
            "INFO:tensorflow:Assets written to: model-005.model/assets\n",
            "Epoch 6/30\n",
            "151/151 [==============================] - 145s 960ms/step - loss: 0.3754 - acc: 0.9002 - val_loss: 0.2899 - val_acc: 0.9447\n",
            "INFO:tensorflow:Assets written to: model-006.model/assets\n",
            "Epoch 7/30\n",
            "151/151 [==============================] - 145s 962ms/step - loss: 0.3557 - acc: 0.9039 - val_loss: 0.2593 - val_acc: 0.9532\n",
            "INFO:tensorflow:Assets written to: model-007.model/assets\n",
            "Epoch 8/30\n",
            "151/151 [==============================] - 145s 958ms/step - loss: 0.3617 - acc: 0.8895 - val_loss: 0.3880 - val_acc: 0.8553\n",
            "Epoch 9/30\n",
            "151/151 [==============================] - 144s 952ms/step - loss: 0.3681 - acc: 0.8852 - val_loss: 0.2368 - val_acc: 0.9532\n",
            "INFO:tensorflow:Assets written to: model-009.model/assets\n",
            "Epoch 10/30\n",
            "151/151 [==============================] - 144s 950ms/step - loss: 0.3070 - acc: 0.9132 - val_loss: 0.3295 - val_acc: 0.8681\n",
            "Epoch 11/30\n",
            "151/151 [==============================] - 143s 946ms/step - loss: 0.2565 - acc: 0.9200 - val_loss: 0.1227 - val_acc: 0.9532\n",
            "INFO:tensorflow:Assets written to: model-011.model/assets\n",
            "Epoch 12/30\n",
            "151/151 [==============================] - 143s 948ms/step - loss: 0.2219 - acc: 0.9209 - val_loss: 0.0965 - val_acc: 0.9745\n",
            "INFO:tensorflow:Assets written to: model-012.model/assets\n",
            "Epoch 13/30\n",
            "151/151 [==============================] - 144s 950ms/step - loss: 0.1969 - acc: 0.9235 - val_loss: 0.1080 - val_acc: 0.9617\n",
            "Epoch 14/30\n",
            "151/151 [==============================] - 143s 945ms/step - loss: 0.2273 - acc: 0.9207 - val_loss: 0.0946 - val_acc: 0.9660\n",
            "INFO:tensorflow:Assets written to: model-014.model/assets\n",
            "Epoch 15/30\n",
            "151/151 [==============================] - 143s 948ms/step - loss: 0.2201 - acc: 0.9159 - val_loss: 0.0828 - val_acc: 0.9681\n",
            "INFO:tensorflow:Assets written to: model-015.model/assets\n",
            "Epoch 16/30\n",
            "151/151 [==============================] - 142s 942ms/step - loss: 0.1595 - acc: 0.9500 - val_loss: 0.1212 - val_acc: 0.9532\n",
            "Epoch 17/30\n",
            "151/151 [==============================] - 142s 943ms/step - loss: 0.1941 - acc: 0.9256 - val_loss: 0.0951 - val_acc: 0.9681\n",
            "Epoch 18/30\n",
            "151/151 [==============================] - 142s 943ms/step - loss: 0.1706 - acc: 0.9365 - val_loss: 0.0869 - val_acc: 0.9702\n",
            "Epoch 19/30\n",
            "151/151 [==============================] - 142s 942ms/step - loss: 0.1599 - acc: 0.9434 - val_loss: 0.0650 - val_acc: 0.9745\n",
            "INFO:tensorflow:Assets written to: model-019.model/assets\n",
            "Epoch 20/30\n",
            "151/151 [==============================] - 143s 947ms/step - loss: 0.1596 - acc: 0.9523 - val_loss: 0.1009 - val_acc: 0.9681\n",
            "Epoch 21/30\n",
            "151/151 [==============================] - 146s 970ms/step - loss: 0.1712 - acc: 0.9400 - val_loss: 0.1066 - val_acc: 0.9553\n",
            "Epoch 22/30\n",
            "151/151 [==============================] - 147s 977ms/step - loss: 0.1802 - acc: 0.9290 - val_loss: 0.0522 - val_acc: 0.9766\n",
            "INFO:tensorflow:Assets written to: model-022.model/assets\n",
            "Epoch 23/30\n",
            "151/151 [==============================] - 147s 973ms/step - loss: 0.1434 - acc: 0.9553 - val_loss: 0.0398 - val_acc: 0.9872\n",
            "INFO:tensorflow:Assets written to: model-023.model/assets\n",
            "Epoch 24/30\n",
            "151/151 [==============================] - 146s 964ms/step - loss: 0.1562 - acc: 0.9494 - val_loss: 0.0862 - val_acc: 0.9596\n",
            "Epoch 25/30\n",
            "151/151 [==============================] - 145s 957ms/step - loss: 0.1276 - acc: 0.9492 - val_loss: 0.0650 - val_acc: 0.9681\n",
            "Epoch 26/30\n",
            "151/151 [==============================] - 145s 961ms/step - loss: 0.1449 - acc: 0.9413 - val_loss: 0.0468 - val_acc: 0.9894\n",
            "Epoch 27/30\n",
            "151/151 [==============================] - 147s 970ms/step - loss: 0.1642 - acc: 0.9373 - val_loss: 0.0463 - val_acc: 0.9872\n",
            "Epoch 28/30\n",
            "151/151 [==============================] - 146s 968ms/step - loss: 0.2266 - acc: 0.9141 - val_loss: 0.0540 - val_acc: 0.9894\n",
            "Epoch 29/30\n",
            "151/151 [==============================] - 146s 969ms/step - loss: 0.1198 - acc: 0.9608 - val_loss: 0.0528 - val_acc: 0.9872\n",
            "Epoch 30/30\n",
            "151/151 [==============================] - 146s 963ms/step - loss: 0.1418 - acc: 0.9493 - val_loss: 0.0753 - val_acc: 0.9787\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggkyK8DY1uKx"
      },
      "source": [
        "## Save"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdzOtkblGwOK"
      },
      "source": [
        "model.save('model.h5')"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkrgvPY-1qN8"
      },
      "source": [
        "## load"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUO-rMxMcgN-"
      },
      "source": [
        "model=tf.keras.models.load_model(\"mymodel.h5\")"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaRbKBZg3Ihq"
      },
      "source": [
        "for Live camera 0 in VideoCapture\n",
        "## OR\n",
        "Path in String \"video.mp4\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86_UDb7I18dh"
      },
      "source": [
        "labels_dict={0:'without_mask',1:'with_mask'}\n",
        "color_dict={0:(0,0,255),1:(0,255,0)}\n",
        "\n",
        "size = 4\n",
        "cap = cv2.VideoCapture(0) #Use camera 0 \n",
        "\n",
        "# We load the xml file\n",
        "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
        "\n",
        "while cap.isOpened():\n",
        "    _,img=cap.read()\n",
        "    face=face_cascade.detectMultiScale(img,scaleFactor=1.1,minNeighbors=4)\n",
        "    for(x,y,w,h) in face:\n",
        "        face_img = img[y:y+h, x:x+w]\n",
        "        cv2.imwrite('temp.jpg',face_img)\n",
        "        test_image=image.load_img('temp.jpg',target_size=(150,150,3))\n",
        "        test_image=image.img_to_array(test_image)\n",
        "        test_image=np.expand_dims(test_image,axis=0)\n",
        "        pred=mymodel.predict_classes(test_image)[0][0]\n",
        "        if pred==1:\n",
        "            cv2.rectangle(img,(x,y),(x+w,y+h),(0,0,255),3)\n",
        "            cv2.putText(img,'NO MASK',((x+w)//2,y+h+20),cv2.FONT_HERSHEY_SIMPLEX,1,(0,0,255),3)\n",
        "        else:\n",
        "            cv2.rectangle(img,(x,y),(x+w,y+h),(0,255,0),3)\n",
        "            cv2.putText(img,'MASK',((x+w)//2,y+h+20),cv2.FONT_HERSHEY_SIMPLEX,1,(0,255,0),3)\n",
        "        datet=str(datetime.datetime.now())\n",
        "        cv2.putText(img,datet,(400,450),cv2.FONT_HERSHEY_SIMPLEX,0.5,(255,255,255),1)\n",
        "    #cv2.imwrite(\"mask_save.mp4\",img)      \n",
        "    cv2.imshow('img',img)\n",
        "    \n",
        "    if cv2.waitKey(1)==ord('q'):\n",
        "      break\n",
        "    \n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}